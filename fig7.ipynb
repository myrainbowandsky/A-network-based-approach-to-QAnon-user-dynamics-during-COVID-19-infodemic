{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8dbc1ddb",
   "metadata": {},
   "source": [
    " # extract Word2Vec embeddings for hashtags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "855cacf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cleantext import clean\n",
    "from tqdm import tqdm\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "import preprocessor as p \n",
    "import hdbscan\n",
    "tqdm.pandas(desc=\"my bar!\")\n",
    "from cleantext import clean\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas(desc=\"my bar!\")\n",
    "import preprocessor as p \n",
    "from nltk import bigrams\n",
    "import networkx as nx\n",
    "import pandas as pd\n",
    "import re\n",
    "from gensim.models import Word2Vec\n",
    "from time import time\n",
    "from matplotlib import rcParams\n",
    "rcParams['figure.figsize'] = 20,20\n",
    "import matplotlib.pyplot as plt\n",
    "import umap\n",
    "import hdbscan\n",
    "\n",
    "def remove_words(string,rex):\n",
    "    words_list = string.split(\",\")\n",
    "    new_list = []\n",
    "    for word in words_list:\n",
    "        if rex.search(word) is None:\n",
    "            # Didn't find a match\n",
    "            new_list.append(word)\n",
    "\n",
    "    new_string = \",\".join(new_list)\n",
    "    new_string=new_string.replace(' ','')\n",
    "\n",
    "    return new_string.upper()\n",
    "def add_hash(_lst):\n",
    "    return ['#'+x for x in _lst if len(_lst)>1]\n",
    "def hashtagsprocess(zdf0):\n",
    "    badwords=[#'qanon',\n",
    "        'deepstate','corona virus','coronavirus',\n",
    "        'covid19','2019-nCoV','SARS-CoV-2','wuhanpneumonia']+['covid','corona','virus']\n",
    "    rex_string = \"(\" + \"|\".join(badwords) + \")\"\n",
    "    rex = re.compile(rex_string, re.IGNORECASE)\n",
    "    \n",
    "    zdf0['hashtags']=zdf0['hashtags'].apply(lambda x : x.strip('[]'))\n",
    "    zdf0['hashtags']=zdf0['hashtags'].apply(lambda x : x.replace(\"'\",\"\"))\n",
    "    zdf0['hashtags']=zdf0['hashtags'].apply(lambda x: remove_words(x,rex))\n",
    "    zdf0['hashtags']=zdf0['hashtags'].str.split(',')\n",
    "    zdf0['hashtags']=zdf0['hashtags'].apply(lambda x: add_hash(x))\n",
    "    return zdf0\n",
    "\n",
    "def prepro(ndf,col):\n",
    "    \n",
    "    import re\n",
    "    def remove_rt(query):\n",
    "        query=re.sub(r\"\\brt\", \"\", query)\n",
    "        return query\n",
    "    ndf[col]=ndf[col].progress_apply(lambda x: remove_rt(x))\n",
    "\n",
    "    def tokenizer(_text):\n",
    "        return TweetTokenizer().tokenize(_text)\n",
    "\n",
    "    ndf[col]=ndf[col].progress_apply(lambda x:tokenizer(x))\n",
    "\n",
    "    from nltk.corpus import stopwords\n",
    "    STOPWORDS=stopwords.words('english')\n",
    "    def remove_stop(_text):\n",
    "        return ' '.join([x for x in _text if x not in STOPWORDS])\n",
    "\n",
    "    ndf[col]=ndf[col].progress_apply(lambda x: remove_stop(x))\n",
    "    \n",
    "    def preprocessing(string): \n",
    "\n",
    "        text=clean(string,lower=True, no_emails=True,no_numbers=True,no_punct=True,no_digits=False,no_currency_symbols=True,\n",
    "                   replace_with_number=\"\",\n",
    "                   no_urls=True,replace_with_url=\"\",\n",
    "                   replace_with_email=\"\",\n",
    "                   replace_with_currency_symbol=\"\")\n",
    "\n",
    "        return text\n",
    "    ndf[col]=ndf[col].progress_apply(lambda x: preprocessing(x))\n",
    "    \n",
    "    def remove_emoji(string):\n",
    "        p.set_options(p.OPT.EMOJI, p.OPT.SMILEY)\n",
    "        \n",
    "        return p.clean(string)\n",
    "    ndf[col]=ndf[col].progress_apply(lambda x: remove_emoji(x))\n",
    "    \n",
    "    ndf.reset_index(inplace=True,drop=True)\n",
    "    return ndf\n",
    "\n",
    "def bihashtagGraph(_wdf0,_i):\n",
    "    terms_bigram = [list(bigrams(tweet)) for tweet in _wdf0['hashtags']]\n",
    "\n",
    "    d=[x for x in terms_bigram if x!=[]]\n",
    "\n",
    "    lst=[]\n",
    "    for each in d:\n",
    "        if len(each)==0:\n",
    "            lst.append(each)\n",
    "        else:\n",
    "            [lst.append(x) for x in each]\n",
    "\n",
    "    hadf=pd.DataFrame(lst)\n",
    "\n",
    "    K=nx.from_pandas_edgelist(hadf,0,1)\n",
    "\n",
    "    nx.write_gexf(K,'../Project3/rebuttle_letter/'+str(_i)+'.gexf')\n",
    "    return K\n",
    "\n",
    "gf=pd.read_csv('../Project3/gf.csv')\n",
    "df=gf[['text','hashtags']]\n",
    "\n",
    "df0=hashtagsprocess(df)\n",
    "df0=prepro(df0,'text')\n",
    "\n",
    "df0['hashtags']=df0['hashtags'].apply(lambda x: ' '.join(x))\n",
    "df0['hashtags']=df0['hashtags'].apply(lambda x: x.lower())\n",
    "df0['whole']=df0['text']+' '+df0['hashtags']\n",
    "df0.drop_duplicates(subset=['whole'],inplace=True)\n",
    "\n",
    "w2v_model = Word2Vec(min_count=1,\n",
    "                     window=5,\n",
    "                     size=300,\n",
    "                     sample=6e-5, \n",
    "                     alpha=0.03, \n",
    "                     min_alpha=0.0007, \n",
    "                     negative=20,\n",
    "                     workers=11)\n",
    "\n",
    "t = time()\n",
    "lst= df0['whole'].tolist()\n",
    "sentences=[x.split() for x in lst]\n",
    "#[for each in x for x in sentences if each!='<not>'|'|']\n",
    "w2v_model.build_vocab(sentences, progress_per=10000)\n",
    "\n",
    "print('Time to build vocab: {} mins'.format(round((time() - t) / 60, 2)))\n",
    "\n",
    "w2v_model.train(sentences, total_examples=w2v_model.corpus_count, epochs=30, report_delay=1)\n",
    "\n",
    "print('Time to train the model: {} mins'.format(round((time() - t) / 60, 2)))\n",
    "\n",
    "w2v_model.wv.most_similar(positive=[\"#qanon\"])\n",
    "\n",
    "df=pd.read_csv('../Project3/gf.csv')\n",
    "df=df[['created_at','hashtags']]\n",
    "df['created_at']=pd.to_datetime(df['created_at']).apply(lambda x: x.strftime('%Y/%m/%d'))\n",
    "df['created_at']=pd.to_datetime(df['created_at'])\n",
    "wdf=df[df['hashtags']!='[]']\n",
    "wdf.sort_values(['created_at'],inplace=True)\n",
    "wdf0=hashtagsprocess(wdf)\n",
    "wdf0['created_at']=pd.to_datetime(wdf0['created_at']).apply(lambda x: x.strftime('%Y/%m'))\n",
    "\n",
    "graphs=[]\n",
    "topics=[]\n",
    "degree=[]\n",
    "topn=30\n",
    "data=pd.DataFrame()\n",
    "for i in [2,3,4,5,6,7]:\n",
    "    ndf=wdf0[wdf0['created_at']=='2020/0'+str(i)]\n",
    "    graph=bihashtagGraph(ndf,i)\n",
    "    graphs.append(graph)\n",
    "    lst=sorted(graph.degree(graph.nodes), key=lambda x: x[1], reverse=True)\n",
    "    top10=[x[0] for x in lst[:topn]]\n",
    "    topdegree=[x[1] for x in lst[:topn]]\n",
    "    topics.append(top10)\n",
    "    degree.append(topdegree)\n",
    "    tmdf=pd.DataFrame([top10,topdegree]).T\n",
    "    tmdf.columns=['topics','degree']\n",
    "    tmdf['date']='2020/0'+str(i)\n",
    "    data=pd.concat([data,tmdf])\n",
    "    \n",
    "\n",
    "\n",
    "data0=data.pivot(\"topics\", \"date\", \"degree\")\n",
    "data0=data0.fillna(0)\n",
    "\n",
    "#ax = sns.heatmap(data0, xticklabels=True,cmap=\"YlGnBu\",cbar_kws={'label': 'degree'})\n",
    "#plt.tight_layout()\n",
    "#plt.xticks(rotation=45)\n",
    "#label_size = 100\n",
    "#plt.rcParams['xtick.labelsize'] = label_size \n",
    "#plt.savefig('../Project3/rebuttle_letter/hashtags_heatmap'+str(topn)+'.pdf')\n",
    "\n",
    "qlst=list(data0.index)\n",
    "qlowerlst=[x.lower() for x in qlst]\n",
    "[w2v_model.n_similarity(['#qanon'],[each]) for each in qlowerlst]\n",
    "\n",
    "ndf=df0[df0['hashtags']!='']\n",
    "\n",
    "ndf['hashtags']=ndf['hashtags'].str.split()\n",
    "\n",
    "ndf0=ndf[['hashtags']]\n",
    "\n",
    "ndf1=ndf0.explode('hashtags')\n",
    "\n",
    "ndf1.drop_duplicates(inplace=True)\n",
    "hashtag_lst=ndf1['hashtags'].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1abe3896",
   "metadata": {},
   "source": [
    "# Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12021b07",
   "metadata": {},
   "outputs": [],
   "source": [
    "#HDBSCAN is used for computing the clustering word2Vec embeddings and UMAP reduces the dimension of those embeddings. \n",
    "umap_args = {\n",
    "    \"n_neighbors\": 15,\n",
    "    \"n_components\": 2, # 5 -> 2 for plotting \n",
    "    \"metric\": \"cosine\",\n",
    "}\n",
    "embeddings=w2v_model[hashtag_lst]\n",
    "umap_model = umap.UMAP(**umap_args).fit(embeddings)\n",
    "\n",
    "#embeddings=w2v_model.wv.vectors\n",
    "print(embeddings.shape)\n",
    "umap_embeddings=umap_model.fit_transform(embeddings)\n",
    "print(umap_embeddings.shape)\n",
    "\n",
    "# 1. HDBSCAN\n",
    "cluster = hdbscan.HDBSCAN(min_cluster_size=15,\n",
    "                          metric='euclidean',                      \n",
    "                          cluster_selection_method='eom').fit(umap_embeddings)\n",
    "# 2. UMAP\n",
    "umap_data = umap.UMAP(n_neighbors=15, n_components=2, min_dist=0.0, metric='cosine').fit_transform(embeddings)\n",
    "result = pd.DataFrame(umap_data, columns=['x', 'y'])\n",
    "\n",
    "result['labels'] = cluster.labels_\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(20, 10))\n",
    "outliers = result.loc[result.labels == -1, :]\n",
    "clustered = result.loc[result.labels != -1, :]\n",
    "plt.scatter(outliers.x, outliers.y, color='#BDBDBD', s=0.05)\n",
    "plt.scatter(clustered.x, clustered.y, c=clustered.labels, s=0.05, cmap='hsv_r')\n",
    "plt.colorbar()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
